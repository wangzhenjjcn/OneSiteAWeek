#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é‡æ–°ç”Ÿæˆå·¥å…·
ä»æ•°æ®åº“å’ŒHTMLæ•°æ®åº“é‡æ–°ç”Ÿæˆdataç›®å½•ä¸‹çš„æ‰€æœ‰é‡‡é›†æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
    python generate_data.py                    # é‡æ–°ç”Ÿæˆæ‰€æœ‰æ•°æ®
    python generate_data.py --limit 10         # é™åˆ¶å¤„ç†10ä¸ªè§†é¢‘
    python generate_data.py --update           # å¼ºåˆ¶æ›´æ–°å·²å­˜åœ¨çš„æ–‡ä»¶
    python generate_data.py --viewkey 123456   # åªå¤„ç†æŒ‡å®šçš„è§†é¢‘ID
    python generate_data.py --stats            # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
"""

import os
import sys
import argparse
from app import PornhubScraper, DatabaseManager, show_database_stats

def main():
    parser = argparse.ArgumentParser(description='ä»æ•°æ®åº“é‡æ–°ç”Ÿæˆdataç›®å½•ä¸‹çš„é‡‡é›†æ–‡ä»¶')
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†çš„è§†é¢‘æ•°é‡')
    parser.add_argument('--update', action='store_true', help='å¼ºåˆ¶æ›´æ–°å·²å­˜åœ¨çš„æ–‡ä»¶')
    parser.add_argument('--viewkey', type=str, help='åªå¤„ç†æŒ‡å®šçš„è§†é¢‘ID')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    parser.add_argument('--source', choices=['html', 'video'], default='html', 
                       help='æ•°æ®æº: html=ä»HTMLæ•°æ®åº“, video=ä»è§†é¢‘æ•°æ®åº“ (é»˜è®¤: html)')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if args.stats:
        print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        show_database_stats()
        return
    
    print("ğŸ”„ æ•°æ®é‡æ–°ç”Ÿæˆå·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–é‡‡é›†å™¨
    scraper = None
    try:
        scraper = PornhubScraper()
        
        if args.source == 'html':
            # ä»HTMLæ•°æ®åº“é‡æ–°ç”Ÿæˆ
            print("ğŸ“‚ ä»HTMLæ•°æ®åº“é‡æ–°ç”Ÿæˆdataç›®å½•...")
            result = generate_from_html_database(scraper, args)
        else:
            # ä»è§†é¢‘æ•°æ®åº“é‡æ–°ç”Ÿæˆ
            print("ğŸ“‚ ä»è§†é¢‘æ•°æ®åº“é‡æ–°ç”Ÿæˆdataç›®å½•...")
            result = generate_from_video_database(scraper, args)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nâœ… é‡æ–°ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  - æˆåŠŸå¤„ç†: {result['success']}")
        print(f"  - å¤„ç†å¤±è´¥: {result['failed']}")
        print(f"  - è·³è¿‡: {result['skipped']}")
        print(f"  - æ€»è®¡: {result['total']}")
        
        if result['failed'] > 0:
            print("\nâš ï¸  å­˜åœ¨å¤„ç†å¤±è´¥çš„è§†é¢‘ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    finally:
        if scraper:
            scraper.close_driver()

def generate_from_html_database(scraper, args):
    """ä»HTMLæ•°æ®åº“é‡æ–°ç”Ÿæˆ"""
    print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"  - æ•°æ®æº: HTMLæ•°æ®åº“")
    print(f"  - å¤„ç†é™åˆ¶: {args.limit or 'æ— é™åˆ¶'}")
    print(f"  - å¼ºåˆ¶æ›´æ–°: {'æ˜¯' if args.update else 'å¦'}")
    print(f"  - æŒ‡å®šè§†é¢‘: {args.viewkey or 'å…¨éƒ¨'}")
    print(f"  - è¯¦ç»†è¾“å‡º: {'æ˜¯' if args.verbose else 'å¦'}")
    
    if args.viewkey:
        # å¤„ç†æŒ‡å®šè§†é¢‘
        return generate_single_video_from_html(scraper, args.viewkey, args.update, args.verbose)
    else:
        # æ‰¹é‡å¤„ç†
        return scraper.regenerate_data_from_html_db(
            limit=args.limit, 
            update_existing=args.update
        )

def generate_from_video_database(scraper, args):
    """ä»è§†é¢‘æ•°æ®åº“é‡æ–°ç”Ÿæˆ"""
    print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"  - æ•°æ®æº: è§†é¢‘æ•°æ®åº“")
    print(f"  - å¤„ç†é™åˆ¶: {args.limit or 'æ— é™åˆ¶'}")
    print(f"  - å¼ºåˆ¶æ›´æ–°: {'æ˜¯' if args.update else 'å¦'}")
    print(f"  - æŒ‡å®šè§†é¢‘: {args.viewkey or 'å…¨éƒ¨'}")
    
    db = DatabaseManager()
    
    # è·å–è§†é¢‘åˆ—è¡¨
    if args.viewkey:
        videos = [db.get_video_by_id(args.viewkey)]
        videos = [v for v in videos if v]  # è¿‡æ»¤None
    else:
        # è·å–æ‰€æœ‰è§†é¢‘
        with db.get_connection() as conn:
            cursor = conn.cursor()
            query = '''
                SELECT v.*, GROUP_CONCAT(c.name) as category_names,
                       GROUP_CONCAT(m.url) as m3u8_urls
                FROM videos v
                LEFT JOIN video_categories vc ON v.id = vc.video_id
                LEFT JOIN categories c ON vc.category_id = c.id
                LEFT JOIN m3u8_urls m ON v.id = m.video_id
                GROUP BY v.id
                ORDER BY v.created_at DESC
            '''
            if args.limit:
                query += f' LIMIT {args.limit}'
            
            cursor.execute(query)
            videos = cursor.fetchall()
    
    if not videos:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ•°æ®")
        return {'success': 0, 'failed': 0, 'skipped': 0, 'total': 0}
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
    
    # å¼€å§‹ä¸‹è½½å·¥ä½œçº¿ç¨‹
    scraper.start_download_workers()
    
    success_count = 0
    failed_count = 0
    skipped_count = 0
    
    try:
        for i, video in enumerate(videos, 1):
            try:
                if args.verbose:
                    print(f"\nğŸ”„ å¤„ç†è§†é¢‘ {i}/{len(videos)}: {video.get('title', 'N/A')[:50]}...")
                
                # è½¬æ¢æ•°æ®åº“æ ¼å¼åˆ°è§†é¢‘æ•°æ®æ ¼å¼
                video_data = convert_db_video_to_data(video)
                
                # æ£€æŸ¥æ˜¯å¦è·³è¿‡
                if not args.update and scraper.is_video_completed(video_data['viewkey']):
                    if args.verbose:
                        print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {video_data['viewkey']}")
                    skipped_count += 1
                    continue
                
                # é‡æ–°ç”Ÿæˆæ–‡ä»¶
                data_folder = os.path.join('data', video_data['viewkey'])
                os.makedirs(data_folder, exist_ok=True)
                
                # åˆ›å»ºHTMLé¡µé¢
                scraper.create_html_page(video_data, data_folder)
                
                # æ·»åŠ ä¸‹è½½ä»»åŠ¡ï¼ˆå¦‚æœæœ‰URLï¼‰
                if video_data.get('thumbnail_url'):
                    thumbnail_path = os.path.join(data_folder, 'thumbnail.jpg')
                    scraper.add_download_task(video_data['thumbnail_url'], thumbnail_path, 'thumbnail')
                
                if video_data.get('preview_url'):
                    preview_path = os.path.join(data_folder, 'preview.webm')
                    scraper.add_download_task(video_data['preview_url'], preview_path, 'preview')
                
                # åˆ›å»ºé‡‡é›†æ—¥å¿—
                scraper.create_collection_log(video_data, data_folder, success=True)
                
                success_count += 1
                
                if args.verbose:
                    print(f"âœ… æˆåŠŸå¤„ç†: {video_data['viewkey']}")
                
            except Exception as e:
                failed_count += 1
                if args.verbose:
                    print(f"âŒ å¤„ç†å¤±è´¥: {e}")
                
        # ç­‰å¾…ä¸‹è½½å®Œæˆ
        print("\nâ³ ç­‰å¾…æ–‡ä»¶ä¸‹è½½å®Œæˆ...")
        scraper.wait_for_downloads()
        
    finally:
        scraper.stop_download_workers()
    
    return {
        'success': success_count,
        'failed': failed_count, 
        'skipped': skipped_count,
        'total': len(videos)
    }

def generate_single_video_from_html(scraper, viewkey, update_existing, verbose):
    """ä»HTMLæ•°æ®åº“å¤„ç†å•ä¸ªè§†é¢‘"""
    # æ„å»ºè§†é¢‘URL
    video_url = f"https://cn.pornhub.com/view_video.php?viewkey={viewkey}"
    
    # ä»HTMLæ•°æ®åº“è·å–
    html_content = scraper.db.get_html_page(video_url)
    if not html_content:
        print(f"âŒ åœ¨HTMLæ•°æ®åº“ä¸­æœªæ‰¾åˆ°è§†é¢‘: {viewkey}")
        return {'success': 0, 'failed': 1, 'skipped': 0, 'total': 1}
    
    print(f"ğŸ”„ å¤„ç†å•ä¸ªè§†é¢‘: {viewkey}")
    
    try:
        # ä½¿ç”¨HTMLé‡æ–°ç”Ÿæˆé€»è¾‘
        from bs4 import BeautifulSoup
        
        # ç¡®ä¿HTMLå†…å®¹æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        if isinstance(html_content, (tuple, list)):
            html_content = html_content[0] if html_content else ""
        elif not isinstance(html_content, str):
            html_content = str(html_content)
            
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–è§†é¢‘ä¿¡æ¯
        video_data = scraper.extract_video_metadata(soup, video_url)
        video_data['viewkey'] = viewkey
        video_data['video_id'] = viewkey
        video_data['video_url'] = video_url
        
        # æ£€æŸ¥è·³è¿‡é€»è¾‘
        if not update_existing:
            file_exists = scraper.is_video_completed(viewkey)
            db_exists = scraper.db.video_exists(viewkey)
            
            if file_exists and db_exists:
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {viewkey}")
                return {'success': 0, 'failed': 0, 'skipped': 1, 'total': 1}
        
        # å¤„ç†è§†é¢‘
        scraper.process_video(video_data)
        
        print(f"âœ… æˆåŠŸå¤„ç†: {viewkey}")
        return {'success': 1, 'failed': 0, 'skipped': 0, 'total': 1}
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        return {'success': 0, 'failed': 1, 'skipped': 0, 'total': 1}

def convert_db_video_to_data(video_row):
    """å°†æ•°æ®åº“è®°å½•è½¬æ¢ä¸ºè§†é¢‘æ•°æ®æ ¼å¼"""
    # å¤„ç†åˆ†ç±»
    categories = []
    if hasattr(video_row, 'category_names') and video_row.category_names:
        for cat_name in video_row.category_names.split(','):
            if cat_name.strip():
                categories.append({'name': cat_name.strip()})
    
    # å¤„ç†M3U8é“¾æ¥
    m3u8_urls = []
    best_m3u8_url = ''
    if hasattr(video_row, 'm3u8_urls') and video_row.m3u8_urls:
        m3u8_urls = [url.strip() for url in video_row.m3u8_urls.split(',') if url.strip()]
        if m3u8_urls:
            best_m3u8_url = m3u8_urls[0]  # ç¬¬ä¸€ä¸ªä½œä¸ºæœ€ä½³è´¨é‡
    
    return {
        'viewkey': video_row.video_id,
        'video_id': video_row.video_id,
        'title': video_row.title or 'N/A',
        'video_url': video_row.original_url or f"https://cn.pornhub.com/view_video.php?viewkey={video_row.video_id}",
        'uploader': video_row.uploader or 'N/A',
        'views': video_row.views or 'N/A',
        'duration': video_row.duration or 'N/A',
        'publish_time': video_row.publish_time or 'N/A',
        'alt_text': f"{video_row.title or 'è§†é¢‘'} ç¼©ç•¥å›¾",
        'categories': categories,
        'thumbnail_url': video_row.thumbnail_url or '',
        'preview_url': video_row.preview_url or '',
        'best_m3u8_url': best_m3u8_url,
        'm3u8_urls': m3u8_urls
    }

if __name__ == "__main__":
    main() 
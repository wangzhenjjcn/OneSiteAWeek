#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pornhubè§†é¢‘æŠ“å–å·¥å…· - ä¼˜åŒ–é‡æ„ç‰ˆæœ¬
ä¿®å¤å¤šçº¿ç¨‹é—®é¢˜ï¼Œæ”¹è¿›èµ„æºç®¡ç†ï¼Œæé«˜ä»£ç è´¨é‡
"""

import requests
import os
import re
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import random
import threading
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
import logging
from typing import Dict, List, Optional, Tuple, Any
import atexit
import signal
import sys

# å¯¼å…¥é…ç½®
from config import (
    PROXY_CONFIG, HEADERS, BASE_URL, SCRAPER_CONFIG, 
    OUTPUT_CONFIG, DEBUG, SSL_CONFIG, SELENIUM_CONFIG, 
    DETAIL_PAGE_CONFIG
)

# Seleniumç›¸å…³å¯¼å…¥
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("è­¦å‘Š: Seleniumä¸å¯ç”¨ï¼Œå°†åªä½¿ç”¨requestsæ¨¡å¼")

# ç¦ç”¨SSLè­¦å‘Š
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO if DEBUG['verbose'] else logging.WARNING,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ResourceManager:
    """èµ„æºç®¡ç†å™¨ - è´Ÿè´£ç»Ÿä¸€ç®¡ç†çº¿ç¨‹æ± ã€è¿æ¥ç­‰èµ„æº"""
    
    def __init__(self):
        self.thread_pools: List[ThreadPoolExecutor] = []
        self.cleanup_callbacks: List[callable] = []
        self._lock = threading.Lock()
        
        # æ³¨å†Œæ¸…ç†å›è°ƒ
        atexit.register(self.cleanup_all)
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        self.cleanup_all()
        sys.exit(0)
    
    def register_thread_pool(self, pool: ThreadPoolExecutor) -> ThreadPoolExecutor:
        """æ³¨å†Œçº¿ç¨‹æ± ä»¥ä¾¿ç»Ÿä¸€ç®¡ç†"""
        with self._lock:
            self.thread_pools.append(pool)
        return pool
    
    def register_cleanup_callback(self, callback: callable):
        """æ³¨å†Œæ¸…ç†å›è°ƒå‡½æ•°"""
        with self._lock:
            self.cleanup_callbacks.append(callback)
    
    def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        with self._lock:
            # æ‰§è¡Œæ¸…ç†å›è°ƒ
            for callback in self.cleanup_callbacks:
                try:
                    callback()
                except Exception as e:
                    logger.error(f"æ¸…ç†å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
            
            # å…³é—­çº¿ç¨‹æ± 
            for pool in self.thread_pools:
                try:
                    pool.shutdown(wait=False)
                except Exception as e:
                    logger.error(f"çº¿ç¨‹æ± å…³é—­å¤±è´¥: {e}")
            
            self.thread_pools.clear()
            self.cleanup_callbacks.clear()


class SafeSession:
    """çº¿ç¨‹å®‰å…¨çš„Sessionç®¡ç†å™¨"""
    
    def __init__(self, proxies=None, headers=None, timeout=30):
        self._local = threading.local()
        self.proxies = proxies
        self.headers = headers
        self.timeout = timeout
    
    def _get_session(self) -> requests.Session:
        """è·å–çº¿ç¨‹æœ¬åœ°çš„Session"""
        if not hasattr(self._local, 'session'):
            session = requests.Session()
            if self.proxies:
                session.proxies.update(self.proxies)
            if self.headers:
                session.headers.update(self.headers)
            session.verify = SSL_CONFIG.get('verify', False)
            self._local.session = session
        return self._local.session
    
    def get(self, url: str, **kwargs) -> requests.Response:
        """çº¿ç¨‹å®‰å…¨çš„GETè¯·æ±‚"""
        session = self._get_session()
        kwargs.setdefault('timeout', self.timeout)
        return session.get(url, **kwargs)
    
    def close_all(self):
        """å…³é—­æ‰€æœ‰Session"""
        if hasattr(self._local, 'session'):
            try:
                self._local.session.close()
                delattr(self._local, 'session')
            except Exception as e:
                logger.error(f"å…³é—­Sessionå¤±è´¥: {e}")


class PornhubScraperOptimized:
    """ä¼˜åŒ–é‡æ„åçš„Pornhubé‡‡é›†å™¨"""
    
    def __init__(self, use_selenium: Optional[bool] = None):
        self.base_url = BASE_URL
        self.resource_manager = ResourceManager()
        
        # åˆå§‹åŒ–Sessionç®¡ç†å™¨
        self.session_manager = SafeSession(
            proxies=PROXY_CONFIG,
            headers=HEADERS,
            timeout=SCRAPER_CONFIG.get('timeout', 60)
        )
        
        # æ³¨å†Œæ¸…ç†å›è°ƒ
        self.resource_manager.register_cleanup_callback(self._cleanup)
        
        # ä¸‹è½½é˜Ÿåˆ—å’Œç»“æœ
        self.download_queue = Queue()
        self.download_results = {}
        self.download_lock = threading.Lock()
        self.download_workers = []
        
        # Seleniumç›¸å…³
        self.use_selenium = use_selenium if use_selenium is not None else SELENIUM_CONFIG.get('use_selenium', True)
        self.driver = None
        self.ad_monitor_thread = None
        self.stop_ad_monitor = threading.Event()
        
        # åˆå§‹åŒ–
        if self.use_selenium and SELENIUM_AVAILABLE:
            self._init_selenium_driver()
        elif self.use_selenium and not SELENIUM_AVAILABLE:
            logger.warning("Seleniumä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°requestsæ¨¡å¼")
            self.use_selenium = False
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("å¼€å§‹æ¸…ç†PornhubScraperèµ„æº...")
        
        # åœæ­¢ä¸‹è½½å·¥ä½œçº¿ç¨‹
        self._stop_download_workers()
        
        # åœæ­¢å¹¿å‘Šç›‘æ§
        if self.ad_monitor_thread and self.ad_monitor_thread.is_alive():
            self.stop_ad_monitor.set()
            self.ad_monitor_thread.join(timeout=2)
        
        # å…³é—­Selenium
        if self.driver:
            try:
                self.driver.quit()
                self.driver = None
            except Exception as e:
                logger.error(f"å…³é—­Seleniumå¤±è´¥: {e}")
        
        # å…³é—­Session
        self.session_manager.close_all()
        
        logger.info("PornhubScraperèµ„æºæ¸…ç†å®Œæˆ")
    
    def _init_selenium_driver(self):
        """åˆå§‹åŒ–Selenium WebDriver"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–Selenium WebDriver...")
            
            # æ£€æµ‹GitHub Actionsç¯å¢ƒ
            is_github_actions = self._is_github_actions_environment()
            
            chrome_options = Options()
            
            # åŸºç¡€é…ç½®
            basic_args = [
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-blink-features=AutomationControlled',
                '--ignore-ssl-errors',
                '--ignore-certificate-errors',
                '--ignore-certificate-errors-spki-list',
                '--disable-web-security',
                '--allow-running-insecure-content',
                '--disable-features=VizDisplayCompositor'
            ]
            
            for arg in basic_args:
                chrome_options.add_argument(arg)
            
            # æ— å¤´æ¨¡å¼
            if SELENIUM_CONFIG.get('headless', True):
                chrome_options.add_argument('--headless')
            
            # çª—å£å¤§å°
            window_size = SELENIUM_CONFIG.get('window_size', '1920,1080')
            chrome_options.add_argument(f'--window-size={window_size}')
            
            # æ€§èƒ½ä¼˜åŒ–
            if SELENIUM_CONFIG.get('disable_images', True):
                prefs = {"profile.managed_default_content_settings.images": 2}
                chrome_options.add_experimental_option("prefs", prefs)
            
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ä»£ç†è®¾ç½®
            if not is_github_actions and PROXY_CONFIG:
                proxy_url = PROXY_CONFIG.get('http', '')
                if proxy_url:
                    chrome_options.add_argument(f'--proxy-server={proxy_url}')
            
            # åˆ›å»ºæœåŠ¡
            try:
                service = Service(ChromeDriverManager().install())
            except Exception:
                # å›é€€åˆ°ç³»ç»ŸPATHä¸­çš„chromedriver
                service = Service()
            
            # åˆ›å»ºé©±åŠ¨
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # è®¾ç½®è¶…æ—¶
            self.driver.set_page_load_timeout(SELENIUM_CONFIG.get('page_load_timeout', 10))
            self.driver.implicitly_wait(SELENIUM_CONFIG.get('implicit_wait', 3))
            
            # æ‰§è¡Œè„šæœ¬éšè—è‡ªåŠ¨åŒ–æ ‡è¯†
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info("Selenium WebDriveråˆå§‹åŒ–æˆåŠŸ")
            
            # å¯åŠ¨å¹¿å‘Šç›‘æ§
            if SELENIUM_CONFIG.get('enable_ad_monitor', True):
                self._start_ad_monitor()
                
        except Exception as e:
            logger.error(f"Seleniumåˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_selenium = False
            self.driver = None
    
    def _is_github_actions_environment(self) -> bool:
        """æ£€æµ‹æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­"""
        github_env_vars = ['GITHUB_ACTIONS', 'GITHUB_WORKSPACE', 'GITHUB_REPOSITORY']
        return any(os.getenv(var) for var in github_env_vars)
    
    def _start_ad_monitor(self):
        """å¯åŠ¨å¹¿å‘Šç›‘æ§çº¿ç¨‹"""
        if self.ad_monitor_thread and self.ad_monitor_thread.is_alive():
            return
        
        self.stop_ad_monitor.clear()
        self.ad_monitor_thread = threading.Thread(target=self._ad_monitor_worker, daemon=True)
        self.ad_monitor_thread.start()
        logger.info("å¹¿å‘Šç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    def _ad_monitor_worker(self):
        """å¹¿å‘Šç›‘æ§å·¥ä½œçº¿ç¨‹"""
        interval = SELENIUM_CONFIG.get('ad_monitor_interval', 5)
        
        while not self.stop_ad_monitor.wait(interval):
            if not self.driver:
                break
            
            try:
                self._close_ad_tabs()
            except Exception as e:
                logger.debug(f"å¹¿å‘Šç›‘æ§å¼‚å¸¸: {e}")
    
    def _close_ad_tabs(self):
        """å…³é—­å¹¿å‘Šæ ‡ç­¾é¡µ"""
        if not self.driver:
            return
        
        try:
            current_handles = self.driver.window_handles
            if len(current_handles) <= 1:
                return
            
            main_handle = current_handles[0]
            ad_handles = current_handles[1:]
            
            for handle in ad_handles:
                try:
                    self.driver.switch_to.window(handle)
                    self.driver.close()
                except Exception as e:
                    logger.debug(f"å…³é—­å¹¿å‘Šæ ‡ç­¾é¡µå¤±è´¥: {e}")
            
            # åˆ‡æ¢å›ä¸»æ ‡ç­¾é¡µ
            self.driver.switch_to.window(main_handle)
            
        except Exception as e:
            logger.debug(f"å¹¿å‘Šæ ‡ç­¾é¡µæ£€æŸ¥å¤±è´¥: {e}")
    
    @contextmanager
    def download_worker_pool(self, num_workers: int = None):
        """ä¸‹è½½å·¥ä½œçº¿ç¨‹æ± ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if num_workers is None:
            num_workers = SCRAPER_CONFIG.get('download_threads', 10)
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        self._start_download_workers(num_workers)
        
        try:
            yield self
        finally:
            # åœæ­¢å·¥ä½œçº¿ç¨‹
            self._stop_download_workers()
    
    def _start_download_workers(self, num_workers: int):
        """å¯åŠ¨ä¸‹è½½å·¥ä½œçº¿ç¨‹"""
        if self.download_workers:
            return  # å·²ç»å¯åŠ¨
        
        for i in range(num_workers):
            worker = threading.Thread(
                target=self._download_worker, 
                args=(i + 1,), 
                daemon=True
            )
            worker.start()
            self.download_workers.append(worker)
        
        logger.info(f"å¯åŠ¨ {num_workers} ä¸ªä¸‹è½½å·¥ä½œçº¿ç¨‹")
    
    def _stop_download_workers(self):
        """åœæ­¢ä¸‹è½½å·¥ä½œçº¿ç¨‹"""
        if not self.download_workers:
            return
        
        # å‘é€åœæ­¢ä¿¡å·
        for _ in self.download_workers:
            self.download_queue.put(None)
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        for worker in self.download_workers:
            worker.join(timeout=2)
        
        self.download_workers.clear()
        logger.info("ä¸‹è½½å·¥ä½œçº¿ç¨‹å·²åœæ­¢")
    
    def _download_worker(self, worker_id: int):
        """ä¸‹è½½å·¥ä½œçº¿ç¨‹"""
        logger.debug(f"ä¸‹è½½å·¥ä½œçº¿ç¨‹ {worker_id} å¯åŠ¨")
        
        while True:
            try:
                task = self.download_queue.get(timeout=1)
                if task is None:  # åœæ­¢ä¿¡å·
                    break
                
                url, filepath, task_type = task
                success = self._download_file(url, filepath)
                
                with self.download_lock:
                    self.download_results[filepath] = {
                        'success': success,
                        'type': task_type,
                        'url': url
                    }
                
                self.download_queue.task_done()
                
            except Empty:
                continue
            except Exception as e:
                logger.error(f"ä¸‹è½½å·¥ä½œçº¿ç¨‹ {worker_id} å¼‚å¸¸: {e}")
                try:
                    self.download_queue.task_done()
                except ValueError:
                    pass
        
        logger.debug(f"ä¸‹è½½å·¥ä½œçº¿ç¨‹ {worker_id} ç»“æŸ")
    
    def _download_file(self, url: str, filepath: str) -> bool:
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(filepath) and os.path.getsize(filepath) > 0:
                return True
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            # ä¸‹è½½æ–‡ä»¶
            response = self.session_manager.get(url, stream=True)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            return True
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ {url}: {e}")
            return False
    
    def add_download_task(self, url: str, filepath: str, task_type: str):
        """æ·»åŠ ä¸‹è½½ä»»åŠ¡"""
        self.download_queue.put((url, filepath, task_type))
    
    def wait_for_downloads(self) -> Dict[str, Any]:
        """ç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆ"""
        self.download_queue.join()
        return self.download_results.copy()
    
    def get_page_requests(self, url: str) -> Optional[str]:
        """ä½¿ç”¨requestsè·å–é¡µé¢å†…å®¹"""
        max_retries = SCRAPER_CONFIG.get('max_retries', 3)
        
        for attempt in range(max_retries):
            try:
                delay = random.uniform(
                    SCRAPER_CONFIG.get('delay_min', 2),
                    SCRAPER_CONFIG.get('delay_max', 5)
                )
                time.sleep(delay)
                
                response = self.session_manager.get(url)
                response.raise_for_status()
                return response.text
                
            except Exception as e:
                logger.warning(f"è·å–é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        return None
    
    def get_page_selenium(self, url: str) -> Optional[str]:
        """ä½¿ç”¨Seleniumè·å–é¡µé¢å†…å®¹"""
        if not self.driver:
            return None
        
        try:
            self.driver.get(url)
            
            # å¤„ç†å¹´é¾„éªŒè¯
            self._handle_age_verification()
            
            return self.driver.page_source
            
        except TimeoutException:
            logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶: {url}")
            try:
                self.driver.refresh()
                time.sleep(5)
                return self.driver.page_source
            except Exception:
                return None
        except Exception as e:
            logger.error(f"Seleniumè·å–é¡µé¢å¤±è´¥: {e}")
            return None
    
    def _handle_age_verification(self):
        """å¤„ç†å¹´é¾„éªŒè¯"""
        try:
            # æŸ¥æ‰¾å¹´é¾„éªŒè¯æŒ‰é’®
            age_button_selectors = [
                "a[href*='age_verified=1']",
                ".age-verification a",
                "#age-verification-btn",
                ".ageVerificationWrapper a"
            ]
            
            for selector in age_button_selectors:
                try:
                    button = WebDriverWait(self.driver, 2).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                    )
                    self.driver.execute_script("arguments[0].click();", button)
                    logger.info("å¹´é¾„éªŒè¯å®Œæˆ")
                    time.sleep(2)
                    return
                except TimeoutException:
                    continue
                    
        except Exception as e:
            logger.debug(f"å¹´é¾„éªŒè¯å¤„ç†: {e}")
    
    def parse_video_list(self, html_content: str) -> List[Dict[str, Any]]:
        """è§£æè§†é¢‘åˆ—è¡¨é¡µé¢"""
        videos = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            video_elements = soup.find_all('li', {'data-entrycode': True})
            
            for li_element in video_elements:
                try:
                    video_info = self._extract_video_info(li_element)
                    if video_info:
                        videos.append(video_info)
                except Exception as e:
                    logger.debug(f"è§£æè§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"è§£æè§†é¢‘åˆ—è¡¨å¤±è´¥: {e}")
        
        return videos
    
    def _extract_video_info(self, li_element) -> Optional[Dict[str, Any]]:
        """ä»liå…ƒç´ ä¸­æå–è§†é¢‘ä¿¡æ¯"""
        try:
            # è·å–viewkey
            viewkey = li_element.get('data-entrycode', '').strip()
            if not viewkey:
                return None
            
            # è·å–æ ‡é¢˜é“¾æ¥
            title_link = li_element.find('a', class_='linkVideoThumb')
            if not title_link:
                return None
            
            title = title_link.get('title', '').strip()
            video_url = urljoin(self.base_url, title_link.get('href', ''))
            
            # è·å–ç¼©ç•¥å›¾
            img_element = li_element.find('img', class_='thumb')
            thumbnail_url = img_element.get('data-src') or img_element.get('src') if img_element else ''
            
            # è·å–æ—¶é•¿
            duration_element = li_element.find('var', class_='duration')
            duration = duration_element.get_text().strip() if duration_element else ''
            
            # è·å–é¢„è§ˆè§†é¢‘
            preview_element = li_element.find('video')
            preview_url = preview_element.get('data-src') if preview_element else ''
            
            return {
                'viewkey': viewkey,
                'title': title,
                'url': video_url,
                'thumbnail_url': thumbnail_url,
                'duration': duration,
                'preview_url': preview_url
            }
            
        except Exception as e:
            logger.debug(f"æå–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def get_video_detailed_info(self, viewkey: str) -> Optional[Dict[str, Any]]:
        """è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯"""
        video_url = f"https://cn.pornhub.com/view_video.php?viewkey={viewkey}"
        
        # ä½¿ç”¨requestsè·å–è¯¦æƒ…é¡µé¢
        html_content = self.get_page_requests(video_url)
        if not html_content:
            return None
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–åŸºæœ¬ä¿¡æ¯
            title_element = soup.find('h1', class_='title')
            title = title_element.get_text().strip() if title_element else f"Video_{viewkey}"
            
            # æå–m3u8é“¾æ¥
            m3u8_urls = self._extract_m3u8_urls(html_content)
            
            # å…¶ä»–ä¿¡æ¯...
            info = {
                'viewkey': viewkey,
                'title': title,
                'url': video_url,
                'm3u8_urls': m3u8_urls,
                # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–å­—æ®µ
            }
            
            return info
            
        except Exception as e:
            logger.error(f"è§£æè§†é¢‘è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _extract_m3u8_urls(self, html_content: str) -> List[str]:
        """æå–m3u8é“¾æ¥"""
        m3u8_urls = []
        
        try:
            # æŸ¥æ‰¾JavaScriptä¸­çš„m3u8é“¾æ¥
            m3u8_pattern = r'https?://[^"\'\s]*\.m3u8[^"\'\s]*'
            matches = re.findall(m3u8_pattern, html_content)
            
            for match in matches:
                # æ¸…ç†URL
                cleaned_url = match.rstrip('",;})')
                if cleaned_url not in m3u8_urls:
                    m3u8_urls.append(cleaned_url)
            
        except Exception as e:
            logger.debug(f"æå–m3u8é“¾æ¥å¤±è´¥: {e}")
        
        return m3u8_urls
    
    def create_html_page(self, video_data: Dict[str, Any], folder_path: str) -> str:
        """åˆ›å»ºHTMLé¡µé¢"""
        try:
            # ç”ŸæˆHTMLå†…å®¹
            html_content = self._generate_html_content(video_data)
            
            # ä¿å­˜HTMLæ–‡ä»¶
            html_filename = OUTPUT_CONFIG.get('html_filename', 'index.html')
            html_filepath = os.path.join(folder_path, html_filename)
            
            os.makedirs(folder_path, exist_ok=True)
            
            with open(html_filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            return html_filepath
            
        except Exception as e:
            logger.error(f"åˆ›å»ºHTMLé¡µé¢å¤±è´¥: {e}")
            return ""
    
    def _generate_html_content(self, video_data: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLå†…å®¹"""
        title = video_data.get('title', 'Unknown Video')
        viewkey = video_data.get('viewkey', '')
        m3u8_urls = video_data.get('m3u8_urls', [])
        
        # é€‰æ‹©æœ€ä½³è´¨é‡çš„m3u8
        best_m3u8_url = m3u8_urls[0] if m3u8_urls else 'N/A'
        
        # ç”Ÿæˆè´¨é‡é“¾æ¥
        quality_links_html = self._generate_quality_links(m3u8_urls)
        
        html_template = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .video-info {{
            background: white;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .video-title {{
            font-size: 24px;
            font-weight: bold;
            color: #333;
            margin-bottom: 15px;
        }}
        .m3u8-links-section {{
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
            border: 2px solid #007bff;
        }}
        .best-quality-btn {{
            display: inline-block;
            padding: 15px 30px;
            background: #28a745;
            color: white;
            text-decoration: none;
            border-radius: 8px;
            font-size: 18px;
            font-weight: bold;
            transition: background 0.3s;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
        }}
        .best-quality-btn:hover {{
            background: #218838;
            text-decoration: none;
            color: white;
        }}
        .quality-links {{
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 15px;
        }}
        .quality-link {{
            display: inline-block;
            padding: 8px 15px;
            background: #007bff;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            font-size: 14px;
            transition: background 0.3s;
        }}
        .quality-link:hover {{
            background: #0056b3;
            text-decoration: none;
            color: white;
        }}
        .no-link {{
            text-align: center;
            color: #6c757d;
            font-style: italic;
            padding: 20px;
        }}
    </style>
</head>
<body>
    <div class="video-info">
        <h1 class="video-title">{title}</h1>
        <p><strong>Viewkey:</strong> {viewkey}</p>
        
        <div class="m3u8-links-section">
            <h3>ğŸ¬ M3U8 è§†é¢‘é“¾æ¥</h3>
            <div style="text-align: center; margin-bottom: 15px;">
                {f'<a href="{best_m3u8_url}" target="_blank" class="best-quality-btn">ğŸ¯ æ‰“å¼€æœ€ä½³è´¨é‡è§†é¢‘</a>' if best_m3u8_url != 'N/A' else '<p class="no-link">æš‚æ— å¯ç”¨çš„m3u8è§†é¢‘é“¾æ¥</p>'}
            </div>
            <div>
                <h4>æ‰€æœ‰å¯ç”¨è´¨é‡:</h4>
                <div class="quality-links">
                    {quality_links_html}
                </div>
            </div>
        </div>
    </div>
</body>
</html>"""
        
        return html_template
    
    def _generate_quality_links(self, m3u8_urls: List[str]) -> str:
        """ç”Ÿæˆè´¨é‡é“¾æ¥HTML"""
        if not m3u8_urls:
            return '<p class="no-link">æš‚æ— å…¶ä»–è´¨é‡å¯ç”¨</p>'
        
        quality_priority = ['1080P', '720P', '480P', '240P', 'HD', 'SD']
        links_html = ""
        
        for i, url in enumerate(m3u8_urls):
            # å°è¯•ä»URLä¸­æå–è´¨é‡ä¿¡æ¯
            quality_name = f"è´¨é‡ {i+1}"
            for priority in quality_priority:
                if priority in url:
                    quality_name = priority
                    break
            
            links_html += f'<a href="{url}" target="_blank" class="quality-link">{quality_name}</a>'
        
        return links_html
    
    def run_optimized(self, start_page: int = 1, max_pages: int = None) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„è¿è¡Œæµç¨‹"""
        logger.info("å¼€å§‹ä¼˜åŒ–é‡‡é›†æµç¨‹...")
        
        try:
            with self.download_worker_pool():
                # é‡‡é›†é¡µé¢å¹¶å¤„ç†è§†é¢‘
                results = self._process_pages(start_page, max_pages)
                
                # ç­‰å¾…ä¸‹è½½å®Œæˆ
                download_results = self.wait_for_downloads()
                results['download_results'] = download_results
                
                return results
                
        except Exception as e:
            logger.error(f"é‡‡é›†æµç¨‹å¤±è´¥: {e}")
            return {}
    
    def _process_pages(self, start_page: int, max_pages: int) -> Dict[str, Any]:
        """å¤„ç†é¡µé¢é‡‡é›†"""
        videos_processed = 0
        total_pages = 0
        
        page = start_page
        while True:
            if max_pages and page > start_page + max_pages - 1:
                break
            
            logger.info(f"å¤„ç†ç¬¬ {page} é¡µ...")
            
            # è·å–é¡µé¢å†…å®¹
            page_url = f"{self.base_url}?page={page}"
            html_content = self.get_page_requests(page_url) or self.get_page_selenium(page_url)
            
            if not html_content:
                logger.warning(f"ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")
                page += 1
                continue
            
            # è§£æè§†é¢‘åˆ—è¡¨
            videos = self.parse_video_list(html_content)
            if not videos:
                logger.info(f"ç¬¬ {page} é¡µæ— è§†é¢‘ï¼Œåœæ­¢é‡‡é›†")
                break
            
            # å¤„ç†è§†é¢‘
            for video in videos:
                self._process_single_video(video)
                videos_processed += 1
            
            total_pages += 1
            page += 1
            
            logger.info(f"ç¬¬ {page-1} é¡µå¤„ç†å®Œæˆï¼Œå…± {len(videos)} ä¸ªè§†é¢‘")
        
        return {
            'total_pages': total_pages,
            'videos_processed': videos_processed
        }
    
    def _process_single_video(self, video_info: Dict[str, Any]):
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        viewkey = video_info.get('viewkey')
        if not viewkey:
            return
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        folder_path = os.path.join(OUTPUT_CONFIG.get('data_folder', 'data'), viewkey)
        if SCRAPER_CONFIG.get('skip_existing', True) and os.path.exists(folder_path):
            logger.debug(f"è·³è¿‡å·²å­˜åœ¨çš„è§†é¢‘: {viewkey}")
            return
        
        # è·å–è¯¦ç»†ä¿¡æ¯
        detailed_info = self.get_video_detailed_info(viewkey)
        if not detailed_info:
            logger.warning(f"è·å–è§†é¢‘è¯¦æƒ…å¤±è´¥: {viewkey}")
            return
        
        # åˆå¹¶ä¿¡æ¯
        video_data = {**video_info, **detailed_info}
        
        # åˆ›å»ºæ–‡ä»¶å¤¹
        os.makedirs(folder_path, exist_ok=True)
        
        # åˆ›å»ºHTMLé¡µé¢
        self.create_html_page(video_data, folder_path)
        
        # æ·»åŠ ä¸‹è½½ä»»åŠ¡
        if video_info.get('thumbnail_url'):
            thumbnail_path = os.path.join(folder_path, OUTPUT_CONFIG.get('thumbnail_filename', 'thumbnail.jpg'))
            self.add_download_task(video_info['thumbnail_url'], thumbnail_path, 'thumbnail')
        
        if video_info.get('preview_url'):
            preview_path = os.path.join(folder_path, OUTPUT_CONFIG.get('preview_filename', 'preview.webm'))
            self.add_download_task(video_info['preview_url'], preview_path, 'preview')


def main():
    """ä¸»å‡½æ•°"""
    scraper = PornhubScraperOptimized()
    
    try:
        results = scraper.run_optimized(
            start_page=SCRAPER_CONFIG.get('start_page', 1),
            max_pages=5  # æµ‹è¯•ç”¨ï¼Œé™åˆ¶é¡µæ•°
        )
        
        logger.info("é‡‡é›†å®Œæˆ!")
        logger.info(f"å¤„ç†é¡µæ•°: {results.get('total_pages', 0)}")
        logger.info(f"å¤„ç†è§†é¢‘: {results.get('videos_processed', 0)}")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­é‡‡é›†")
    except Exception as e:
        logger.error(f"é‡‡é›†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main() 